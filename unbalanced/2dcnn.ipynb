{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, balanced_accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "import torch.optim as optim\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mO Kernel deu pane ao executar o código na célula atual ou em uma célula anterior. \n",
      "\u001b[1;31mAnalise o código nas células para identificar uma possível causa da pane. \n",
      "\u001b[1;31mClique <a href='https://aka.ms/vscodeJupyterKernelCrash'>aqui</a> para obter mais informações. \n",
      "\u001b[1;31mConsulte Jupyter <a href='command:jupyter.viewOutput'>log</a> para obter mais detalhes."
     ]
    }
   ],
   "source": [
    "df_polars_raiz = pl.read_parquet('../dataset.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_polars = df_polars_raiz.sample(fraction=0.01, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipaddress\n",
    "\n",
    "def ip_to_int(ip: str) -> int:\n",
    "    try:\n",
    "        return int(ipaddress.ip_address(ip))  # Funciona tanto para IPv4 quanto IPv6\n",
    "    except ValueError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_polars = df_polars.with_columns([\n",
    "#    pl.col('id.resp_h').map_elements(ip_to_int).alias('id.resp_h'),\n",
    "#    pl.col('id.orig_h').map_elements(ip_to_int).alias('id.orig_h')\n",
    "#])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_polars = df_polars.with_columns([\n",
    "    pl.col('duration').fill_null(0),\n",
    "    pl.col('orig_bytes').fill_null(0),\n",
    "    pl.col('resp_bytes').fill_null(0)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_colunas = df_polars.columns\n",
    "colunas_para_spearman = ['id.resp_p', 'history', 'conn_state', 'id.orig_p', 'orig_ip_bytes', 'label']             \n",
    "#['detailed-label', 'id.resp_p', 'history', 'id.orig_h', 'conn_state', 'id.orig_p', 'orig_ip_bytes']\n",
    "colunas_para_dropar = [col for col in lista_colunas if col not in colunas_para_spearman]\n",
    "df_polars = df_polars.drop(colunas_para_dropar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_polars = df_polars.drop_nulls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_polars.drop('label')\n",
    "y = df_polars['label']       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.to_numpy()\n",
    "X[:, 0] = np.array([ip_to_int(ip) for ip in X[:, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcular número desejado de características\n",
    "def calculate_desired_num_features(current_num_features):\n",
    "    root = math.sqrt(current_num_features)\n",
    "    desired_num_features = math.ceil(root) ** 2\n",
    "    \n",
    "    return desired_num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "SAMPLE_2D_SIZE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.2):\n",
    "        super(CNNModel, self).__init__()\n",
    "        \n",
    "        # Camadas Convolucionais\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=2, stride=1, padding=0)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=2, stride=1, padding=0)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        # Camada de Pooling\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Calcula o tamanho da saída após as camadas convolucionais e de pooling\n",
    "        def conv2d_out_size(size, kernel_size=2, stride=1, padding=0):\n",
    "            return (size - kernel_size + 2 * padding) // stride + 1\n",
    "        \n",
    "        size_after_conv1 = conv2d_out_size(SAMPLE_2D_SIZE, kernel_size=2)\n",
    "        size_after_conv2 = conv2d_out_size(size_after_conv1, kernel_size=2)\n",
    "        size_after_pool = conv2d_out_size(size_after_conv2, kernel_size=2, stride=2)\n",
    "        \n",
    "        # Camada Fully Connected\n",
    "        self.fc1 = nn.Linear(64 * size_after_pool * size_after_pool, 64)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def startTrain():\n",
    "    kfold = KFold(n_splits=5, shuffle=True)\n",
    "    results_fold = []\n",
    "    num_epochs = 10  # Número de épocas definido\n",
    "    \n",
    "    for train_idx, test_idx in kfold.split(X, y):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        scaler = MinMaxScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)  # Fit apenas no treino\n",
    "\n",
    "        # Fazer a transformação dos dados de teste com o mesmo scaler (sem \"fitar\" novamente)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        # Calcular o número desejado de características e realizar o preenchimento com zeros se necessário\n",
    "        desired_num_features = 9  # 4x4 = 16 características\n",
    "        current_num_features = X_train_scaled.shape[1]\n",
    "\n",
    "        if current_num_features < desired_num_features:\n",
    "            padding = desired_num_features - current_num_features\n",
    "            X_train_scaled = np.pad(X_train_scaled, ((0, 0), (0, padding)), 'constant')  # Preencher com zeros\n",
    "            X_test_scaled = np.pad(X_test_scaled, ((0, 0), (0, padding)), 'constant')  # Preencher com zeros\n",
    "        elif current_num_features > desired_num_features:\n",
    "            raise ValueError(\"Número de características é maior que o desejado.\")\n",
    "\n",
    "        # Realizar o reshape para 4x4\n",
    "        X_train_scaled = X_train_scaled.reshape(-1, 1, SAMPLE_2D_SIZE, SAMPLE_2D_SIZE)  # 1 canal, 4x4\n",
    "        X_test_scaled = X_test_scaled.reshape(-1, 1, SAMPLE_2D_SIZE, SAMPLE_2D_SIZE)    # 1 canal, 4x4\n",
    "\n",
    "        # Converter para tensores\n",
    "        X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "        y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "        X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "        y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "        # Inicializar o modelo e configurá-lo para o dispositivo (GPU/CPU)\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model = CNNModel(dropout_rate=0.2)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "        # DataLoader\n",
    "        batch_size = 512\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "        start_training = time.time()\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        end_training = time.time()\n",
    "\n",
    "        # Avaliação\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                outputs = model(X_batch)\n",
    "                preds = torch.sigmoid(outputs)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "        all_preds = np.array(all_preds)\n",
    "        all_labels = np.array(all_labels)\n",
    "        y_pred = (all_preds >= 0.5).astype(int)\n",
    "        y_true = all_labels\n",
    "\n",
    "        # Cálculo das métricas\n",
    "        confusion = confusion_matrix(y_true, y_pred)\n",
    "        tn, fp, fn, tp = confusion.ravel()\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        precision = precision_score(y_true, y_pred)\n",
    "        recall = recall_score(y_true, y_pred)\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "        balanced_accuracy = balanced_accuracy_score(y_true, y_pred)\n",
    "        false_alarm_rate = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "\n",
    "        # Cálculo de tempos\n",
    "        training_duration = end_training - start_training\n",
    "        evaluation_duration = time.time() - end_training\n",
    "\n",
    "        # Registro de resultados\n",
    "        avaliacao = [\n",
    "            accuracy, balanced_accuracy, precision, recall, specificity, f1, \n",
    "            false_alarm_rate, tn, fp, fn, tp, training_duration, evaluation_duration\n",
    "        ]\n",
    "        \n",
    "        #print(avaliacao)\n",
    "        results_fold.append(avaliacao)\n",
    "        results_fold_array = np.array(results_fold, dtype=np.float32)\n",
    "    mean_results = np.mean(results_fold_array, axis=0)\n",
    "    results.append([\"2DCNN\"] + mean_results.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,2):\n",
    "    startTrain()\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pl.DataFrame(\n",
    "    results,\n",
    "    schema=['Algorithm', 'Accuracy', 'Balanced Accuracy' , 'Precision', 'Recall', 'Specificity', 'F1-score', 'False Alarm Rate', 'tn', 'fp', 'fn', 'tp', 'training_duration', 'evaluation_duration']\n",
    ")\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.write_csv(f\"metrics_results/unbalanced_2DCNN_metrics_output.csv\", separator=';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tcc_2025_py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
