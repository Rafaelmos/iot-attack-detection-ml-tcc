{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, balanced_accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_polars_raiz = pl.read_parquet('../dataset.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_polars = df_polars_raiz.sample(fraction=0.01, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipaddress\n",
    "\n",
    "def ip_to_int(ip: str) -> int:\n",
    "    try:\n",
    "        return int(ipaddress.ip_address(ip))  # Funciona tanto para IPv4 quanto IPv6\n",
    "    except ValueError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_polars = df_polars.with_columns([\n",
    "#    pl.col('id.resp_h').map_elements(ip_to_int).alias('id.resp_h'),\n",
    "#    pl.col('id.orig_h').map_elements(ip_to_int).alias('id.orig_h')\n",
    "#])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_polars = df_polars.with_columns([\n",
    "    pl.col('duration').fill_null(0),\n",
    "    pl.col('orig_bytes').fill_null(0),\n",
    "    pl.col('resp_bytes').fill_null(0)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_colunas = df_polars.columns\n",
    "colunas_para_spearman = ['id.resp_p', 'history', 'conn_state', 'id.orig_p', 'orig_ip_bytes', 'label']             \n",
    "#['detailed-label', 'id.resp_p', 'history', 'id.orig_h', 'conn_state', 'id.orig_p', 'orig_ip_bytes']\n",
    "colunas_para_dropar = [col for col in lista_colunas if col not in colunas_para_spearman]\n",
    "df_polars = df_polars.drop(colunas_para_dropar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_polars = df_polars.drop_nulls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_polars.drop('label')\n",
    "y = df_polars['label']       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = X.to_numpy()\n",
    "#X[:, 0] = np.array([ip_to_int(ip) for ip in X[:, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=32):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 8),\n",
    "            nn.ReLU(), \n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(8, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim) \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def startTrain(X, y, hidden_dim=8, epochs=10, learning_rate=0.001, batch_size = 5000):\n",
    "    kfold = KFold(n_splits=5, shuffle=True)\n",
    "    results_fold = []\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(kfold.split(X)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        # Aplicar MinMaxScaler APENAS aos dados de treino\n",
    "        scaler = MinMaxScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # Filtrar apenas os dados benignos (classe 0) para treinar o Autoencoder\n",
    "        X_train = X_train[y_train == 0]\n",
    "\n",
    "        # Converter para tensores PyTorch\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "        y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "        # Criar DataLoaders\n",
    "        batch_size = batch_size\n",
    "        train_dataset = TensorDataset(X_train_tensor)\n",
    "        test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # Configuração do dispositivo e modelo\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        input_dim = X_train.shape[1]\n",
    "        model = Autoencoder(input_dim, hidden_dim).to(device)\n",
    "\n",
    "        # Definição da função de perda e otimizador\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Treinamento\n",
    "        epochs = epochs\n",
    "        start_training = time.time()\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            epoch_loss = 0\n",
    "            for data in train_loader:\n",
    "                inputs = data[0].to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, inputs)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        end_training = time.time()\n",
    "\n",
    "        # Avaliação do limiar baseado no conjunto de treino\n",
    "        model.eval()\n",
    "        reconstruction_errors_train = []\n",
    "        with torch.no_grad():\n",
    "            for data in train_loader:\n",
    "                inputs = data[0].to(device)\n",
    "                outputs = model(inputs)\n",
    "                reconstruction_error = torch.mean((outputs - inputs) ** 2, dim=1)\n",
    "                reconstruction_errors_train.extend(reconstruction_error.cpu().numpy())\n",
    "\n",
    "        reconstruction_errors_train = np.array(reconstruction_errors_train)\n",
    "        threshold = np.percentile(reconstruction_errors_train, 95)  # Limiar baseado no percentil 95\n",
    "\n",
    "        # Avaliação no conjunto de teste\n",
    "        reconstruction_errors_test = []\n",
    "        y_pred = []\n",
    "        with torch.no_grad():\n",
    "            for data in test_loader:\n",
    "                inputs, targets = data[0].to(device), data[1].cpu().numpy()\n",
    "                outputs = model(inputs)\n",
    "                reconstruction_error = torch.mean((outputs - inputs) ** 2, dim=1).cpu().numpy()\n",
    "                reconstruction_errors_test.extend(reconstruction_error)\n",
    "                y_pred.extend((reconstruction_error > threshold).astype(int))\n",
    "\n",
    "        # Conversão para numpy arrays\n",
    "        reconstruction_errors_test = np.array(reconstruction_errors_test)\n",
    "        y_pred = np.array(y_pred)\n",
    "\n",
    "        # Cálculo de métricas\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        results_fold.append(accuracy)\n",
    "\n",
    "    return np.mean(results_fold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(X, y):\n",
    "    param_grid = {\n",
    "        'hidden_dim': [8, 16],\n",
    "        'epochs': [10, 20, 25],\n",
    "        'learning_rate': [0.001, 0.0001],\n",
    "        'batch_size': [128, 256, 512, 5000]  # Adicionando o parâmetro batch_size\n",
    "    }\n",
    "\n",
    "    best_accuracy = 0\n",
    "    best_params = {}\n",
    "\n",
    "    for hidden_dim in param_grid['hidden_dim']:\n",
    "        for epochs in param_grid['epochs']:\n",
    "            for learning_rate in param_grid['learning_rate']:\n",
    "                for batch_size in param_grid['batch_size']:  # Iterando também sobre o batch_size\n",
    "                    accuracy = startTrain(X, y, hidden_dim=hidden_dim, \n",
    "                                          epochs=epochs, learning_rate=learning_rate, batch_size=batch_size)\n",
    "                    print(f\"hidden_dim={hidden_dim}, epochs={epochs}, learning_rate={learning_rate}, batch_size={batch_size} -> Accuracy: {accuracy}\")\n",
    "\n",
    "                    if accuracy > best_accuracy:\n",
    "                        best_accuracy = accuracy\n",
    "                        best_params = {\n",
    "                            'hidden_dim': hidden_dim,\n",
    "                            'epochs': epochs,\n",
    "                            'learning_rate': learning_rate,\n",
    "                            'batch_size': batch_size\n",
    "                        }\n",
    "\n",
    "    print(\"\\nMelhores Hiperparâmetros encontrados:\", best_params)\n",
    "    print(f\"Melhor Acurácia Média: {best_accuracy}\")\n",
    "    return best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_dim=8, epochs=10, learning_rate=0.001, batch_size=128 -> Accuracy: 0.9807702389090565\n",
      "hidden_dim=8, epochs=10, learning_rate=0.001, batch_size=256 -> Accuracy: 0.9812259051514257\n",
      "hidden_dim=8, epochs=10, learning_rate=0.001, batch_size=512 -> Accuracy: 0.9522237613508484\n",
      "hidden_dim=8, epochs=10, learning_rate=0.001, batch_size=5000 -> Accuracy: 0.5228888059386978\n",
      "hidden_dim=8, epochs=10, learning_rate=0.0001, batch_size=128 -> Accuracy: 0.9082403045629786\n",
      "hidden_dim=8, epochs=10, learning_rate=0.0001, batch_size=256 -> Accuracy: 0.8720662803328324\n",
      "hidden_dim=8, epochs=10, learning_rate=0.0001, batch_size=512 -> Accuracy: 0.833244735664201\n",
      "hidden_dim=8, epochs=10, learning_rate=0.0001, batch_size=5000 -> Accuracy: 0.3850612932527584\n",
      "hidden_dim=8, epochs=20, learning_rate=0.001, batch_size=128 -> Accuracy: 0.9724367690629551\n",
      "hidden_dim=8, epochs=20, learning_rate=0.001, batch_size=256 -> Accuracy: 0.9772239591844121\n",
      "hidden_dim=8, epochs=20, learning_rate=0.001, batch_size=512 -> Accuracy: 0.9625957880811253\n",
      "hidden_dim=8, epochs=20, learning_rate=0.001, batch_size=5000 -> Accuracy: 0.826078059217104\n",
      "hidden_dim=8, epochs=20, learning_rate=0.0001, batch_size=128 -> Accuracy: 0.9813141458705458\n",
      "hidden_dim=8, epochs=20, learning_rate=0.0001, batch_size=256 -> Accuracy: 0.8450736318682763\n",
      "hidden_dim=8, epochs=20, learning_rate=0.0001, batch_size=512 -> Accuracy: 0.9223245711213857\n",
      "hidden_dim=8, epochs=20, learning_rate=0.0001, batch_size=5000 -> Accuracy: 0.2418911683386522\n",
      "hidden_dim=8, epochs=25, learning_rate=0.001, batch_size=128 -> Accuracy: 0.8739118065760401\n",
      "hidden_dim=8, epochs=25, learning_rate=0.001, batch_size=256 -> Accuracy: 0.981099829277213\n",
      "hidden_dim=8, epochs=25, learning_rate=0.001, batch_size=512 -> Accuracy: 0.980123659429945\n",
      "hidden_dim=8, epochs=25, learning_rate=0.001, batch_size=5000 -> Accuracy: 0.9010687586148982\n",
      "hidden_dim=8, epochs=25, learning_rate=0.0001, batch_size=128 -> Accuracy: 0.9803451881110975\n",
      "hidden_dim=8, epochs=25, learning_rate=0.0001, batch_size=256 -> Accuracy: 0.948497408911323\n",
      "hidden_dim=8, epochs=25, learning_rate=0.0001, batch_size=512 -> Accuracy: 0.8778767812853818\n",
      "hidden_dim=8, epochs=25, learning_rate=0.0001, batch_size=5000 -> Accuracy: 0.3483258784251658\n",
      "hidden_dim=16, epochs=10, learning_rate=0.001, batch_size=128 -> Accuracy: 0.9761902019821216\n",
      "hidden_dim=16, epochs=10, learning_rate=0.001, batch_size=256 -> Accuracy: 0.9644040336433065\n",
      "hidden_dim=16, epochs=10, learning_rate=0.001, batch_size=512 -> Accuracy: 0.9833889533436991\n",
      "hidden_dim=16, epochs=10, learning_rate=0.001, batch_size=5000 -> Accuracy: 0.8777269126722587\n",
      "hidden_dim=16, epochs=10, learning_rate=0.0001, batch_size=128 -> Accuracy: 0.9819535191492827\n",
      "hidden_dim=16, epochs=10, learning_rate=0.0001, batch_size=256 -> Accuracy: 0.9226523892388135\n",
      "hidden_dim=16, epochs=10, learning_rate=0.0001, batch_size=512 -> Accuracy: 0.8795532504343508\n",
      "hidden_dim=16, epochs=10, learning_rate=0.0001, batch_size=5000 -> Accuracy: 0.461808494576781\n",
      "hidden_dim=16, epochs=20, learning_rate=0.001, batch_size=128 -> Accuracy: 0.9820363772676585\n",
      "hidden_dim=16, epochs=20, learning_rate=0.001, batch_size=256 -> Accuracy: 0.9830647688827849\n",
      "hidden_dim=16, epochs=20, learning_rate=0.001, batch_size=512 -> Accuracy: 0.9835492425238183\n",
      "hidden_dim=16, epochs=20, learning_rate=0.001, batch_size=5000 -> Accuracy: 0.8614782298609864\n",
      "hidden_dim=16, epochs=20, learning_rate=0.0001, batch_size=128 -> Accuracy: 0.9822506982887391\n",
      "hidden_dim=16, epochs=20, learning_rate=0.0001, batch_size=256 -> Accuracy: 0.9815771064329673\n",
      "hidden_dim=16, epochs=20, learning_rate=0.0001, batch_size=512 -> Accuracy: 0.8406108845254696\n",
      "hidden_dim=16, epochs=20, learning_rate=0.0001, batch_size=5000 -> Accuracy: 0.26699965109556806\n",
      "hidden_dim=16, epochs=25, learning_rate=0.001, batch_size=128 -> Accuracy: 0.9814204139628055\n",
      "hidden_dim=16, epochs=25, learning_rate=0.001, batch_size=256 -> Accuracy: 0.970291793964779\n",
      "hidden_dim=16, epochs=25, learning_rate=0.001, batch_size=512 -> Accuracy: 0.9704160664967144\n",
      "hidden_dim=16, epochs=25, learning_rate=0.001, batch_size=5000 -> Accuracy: 0.8350794575211609\n",
      "hidden_dim=16, epochs=25, learning_rate=0.0001, batch_size=128 -> Accuracy: 0.9826127038592156\n",
      "hidden_dim=16, epochs=25, learning_rate=0.0001, batch_size=256 -> Accuracy: 0.981726592281064\n",
      "hidden_dim=16, epochs=25, learning_rate=0.0001, batch_size=512 -> Accuracy: 0.8888090668450415\n",
      "hidden_dim=16, epochs=25, learning_rate=0.0001, batch_size=5000 -> Accuracy: 0.29649166511857816\n",
      "\n",
      "Melhores Hiperparâmetros encontrados: {'hidden_dim': 16, 'epochs': 20, 'learning_rate': 0.001, 'batch_size': 512}\n",
      "Melhor Acurácia Média: 0.9835492425238183\n"
     ]
    }
   ],
   "source": [
    "inicio = time.time()\n",
    "best_params = grid_search(X, y)\n",
    "fim = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3570.3299725055695\n"
     ]
    }
   ],
   "source": [
    "duracao = fim - inicio\n",
    "print(duracao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Melhores Hiperparâmetros encontrados: {'hidden_dim': 16, 'epochs': 20, 'learning_rate': 0.001, 'batch_size': 512}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nMelhores Hiperparâmetros encontrados:\", best_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tcc_2025_py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
